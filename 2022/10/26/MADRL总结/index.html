<!DOCTYPE html><html lang="cn" data-theme="light"><head><!-- hexo injector head_begin start --><style type="text/css">.douban-card-block {
    display: flex;
    justify-content: center;
    align-items: center;
    width: 100%;
    max-height: 400px;
}

.douban-card {
    display: flex;
    margin: 30px 10px;
    padding: 15px;
    border-radius: 15px;
    position: relative;
    justify-content: center;
    align-items: center;
    overflow: hidden;
    color: antiquewhite;
    text-decoration: none;
}

.douban-card:hover {
    text-decoration: none;
}

.douban-card-bgimg {
    position: absolute;
    width: 115%;
    height: 115%;
    filter: blur(15px) brightness(0.6);
    background-size: 100%;
    background-position: center;
    background-repeat: no-repeat;
}

.douban-card-img {
    position: relative;
    height: 130px;
    width: 80px;
    background-size: 100%;
    background-position: center;
    background-repeat: no-repeat;
}

.douban-card-left:hover .douban-card-img {
    filter: blur(5px) brightness(0.6);
    transform: perspective(800px) rotateX(180deg);
}

.douban-card-left .douban-card-img {
    transition: all 500ms ease;
}

.douban-card-left {
    position: relative;
    display: flex;
    flex-direction: column;
    align-items: center;
}

.douban-card-left .douban-card-status {
    height: 130px;
    width: 80px;
    text-align: center;
    font-weight: bold;
    position: absolute;
    left: 0;
    top: 30%;
    transform: rotateX(180deg);
    backface-visibility: hidden;
    transition: all 500ms ease;
}

.douban-card-left:hover .douban-card-status {
    transform: perspective(800px) rotateX(0deg);
}

.douban-card-right {
    position: relative;
    display: flex;
    flex-direction: column;
    margin-left: 12px;
    font-size: 16px;
    font-family: "Courier New", Courier, monospace;
    line-height: 1.3;
    color: antiquewhite;
}

.douban-card-item {
    margin-top: 4px;
}
</style><!-- hexo injector head_begin end --><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>MADRL总结 | 奔三啦</title><meta name="author" content="郑晓东"><meta name="copyright" content="郑晓东"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Deep Reinforcement Learning for Multiagent Systems: A Review of Challenges, Solutions, and Applications by Thanh Thi Nguyen"><link rel="shortcut icon" href="/pics/favicon.png"><link rel="canonical" href="http://www.zhengxiaodong.com/2022/10/26/MADRL%E6%80%BB%E7%BB%93/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//fonts.googleapis.com" crossorigin=""/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web&amp;display=swap" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: {"limitDay":730,"position":"top","messagePrev":"It has been","messageNext":"days since the last update, memory was good, but the future is more important :)."},
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: 'days',
  date_suffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: {"limitCount":50,"languages":{"author":"Author: 郑晓东","link":"Link: ","source":"Source: 奔三啦","info":"Copyright is owned by the author. For commercial reprints, please contact the author for authorization. For non-commercial reprints, please indicate the source."}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'MADRL总结',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-05-06 19:14:54'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/essay_page/home_essay_bar.css"><link rel="stylesheet" href="/css/essay_page/essay_page.css"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper-anzhiyu@1.0.4/lib/swiper.min.css"><link rel="stylesheet" href="/css/custom.css" media="defer" onload="this.media='all'"><link rel="stylesheet" href="/css/swiperstyle.css" media="defer" onload="this.media='all'"><link rel="stylesheet" href="/css/sheldon_color.css" media="defer" onload="this.media='all'"><style type="text/css">#toggle-sidebar {left:100px}</style><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/js-heo@1.0.11/poem/poem.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/js-heo@1.0.11/mainColor/heoMainColor.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/swiper/swiper-bundle.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/js-heo@1.0.11/bb/showbb_in_index.css"><!-- hexo injector head_end start --><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-double-row-display@1.00/cardlistpost.min.css"/>
<style>#recent-posts > .recent-post-item >.recent-post-info > .article-meta-wrap > .tags:before {content:"\A";
  white-space: pre;}#recent-posts > .recent-post-item >.recent-post-info > .article-meta-wrap > .tags > .article-meta__separator{display:none}</style>
<link rel="stylesheet" href="https://unpkg.zhimg.com/hexo-butterfly-clock/lib/clock.min.css" /><link rel="stylesheet" href="https://cdn.cbd.int/hexo-butterfly-swiper-anzhiyu-pro/lib/swiper.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://files-1302790708.cos.ap-chengdu.myqcloud.com/swiperstyle.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.cbd.int/hexo-butterfly-swiper-anzhiyu-pro/lib/categoryGroup.css" media="print" onload="this.media='all'"><!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.4.2"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/pics/loren.PNG" onerror="onerror=null;src='/pics/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">36</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">20</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">5</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-project-diagram"></i><span> 项目</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/projects/"><i class="fa-fw fa-solid fa-robot"></i><span> 智能消防机器人</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><span> 教学</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/AI/"><i class="fa-fw fa-solid fa-robot"></i><span> 人工智能基础</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://workspace.jianguoyun.com/inbox/collect/8783efb423aa46838b1192b4df1ddfba/submitv2"><i class="fa-fw fa-solid fa-book"></i><span> 人工智能作业收集</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/%E8%AF%B4%E8%AF%B4/"><i class="fa-fw fa fa-paper-plane"></i><span> 说说</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 存货</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/essay/"><i class="fa-fw fa fa-book"></i><span> 小短文</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></li><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 全部分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 全部标签</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 闲杂时光</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page child" href="/books/"><i class="fa-fw fas fa-book"></i><span> 读书</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li><li><a class="site-page child" href="/games/"><i class="fa-fw fas fa-gamepad"></i><span> 游戏</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 科研</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" target="_blank" rel="noopener" href="https://bir-rl.notion.site/d963eb3e5e6a4dd7ac4dc852e0475e38?v=80f0e820db094cdca799463a6f6d74fc&amp;pvs=4"><i class="fa-fw fa-solid fa-circle-nodes"></i><span> 多智能体强化学习</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://bir-rl.notion.site/9df6c9a31148403c8e37dd7cd3692bf3?v=f3e07b97e1304819b949dadcecc0bdf5&amp;pvs=4"><i class="fa-fw fa-solid fa-book"></i><span> 论文笔记</span></a></li><li><a class="site-page child" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 个人简历</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/%E8%8B%B1%E6%96%87%E7%AE%80%E5%8E%86/"><i class="fa-fw fas fa-heart"></i><span> English</span></a></li><li><a class="site-page child" href="/%E4%B8%AD%E6%96%87%E7%AE%80%E5%8E%86/"><i class="fa-fw fas fa-heart"></i><span> 中文</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于我</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://image-1302790708.cos.ap-chengdu.myqcloud.com/20221106221145.png')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">奔三啦</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-project-diagram"></i><span> 项目</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/projects/"><i class="fa-fw fa-solid fa-robot"></i><span> 智能消防机器人</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><span> 教学</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/AI/"><i class="fa-fw fa-solid fa-robot"></i><span> 人工智能基础</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://workspace.jianguoyun.com/inbox/collect/8783efb423aa46838b1192b4df1ddfba/submitv2"><i class="fa-fw fa-solid fa-book"></i><span> 人工智能作业收集</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/%E8%AF%B4%E8%AF%B4/"><i class="fa-fw fa fa-paper-plane"></i><span> 说说</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 存货</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/essay/"><i class="fa-fw fa fa-book"></i><span> 小短文</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></li><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 全部分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 全部标签</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 闲杂时光</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page child" href="/books/"><i class="fa-fw fas fa-book"></i><span> 读书</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li><li><a class="site-page child" href="/games/"><i class="fa-fw fas fa-gamepad"></i><span> 游戏</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 科研</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" target="_blank" rel="noopener" href="https://bir-rl.notion.site/d963eb3e5e6a4dd7ac4dc852e0475e38?v=80f0e820db094cdca799463a6f6d74fc&amp;pvs=4"><i class="fa-fw fa-solid fa-circle-nodes"></i><span> 多智能体强化学习</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://bir-rl.notion.site/9df6c9a31148403c8e37dd7cd3692bf3?v=f3e07b97e1304819b949dadcecc0bdf5&amp;pvs=4"><i class="fa-fw fa-solid fa-book"></i><span> 论文笔记</span></a></li><li><a class="site-page child" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 个人简历</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/%E8%8B%B1%E6%96%87%E7%AE%80%E5%8E%86/"><i class="fa-fw fas fa-heart"></i><span> English</span></a></li><li><a class="site-page child" href="/%E4%B8%AD%E6%96%87%E7%AE%80%E5%8E%86/"><i class="fa-fw fas fa-heart"></i><span> 中文</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于我</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">MADRL总结</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2022-10-26T13:36:09.000Z" title="Created 2022-10-26 21:36:09">2022-10-26</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2023-05-06T11:14:54.000Z" title="Updated 2023-05-06 19:14:54">2023-05-06</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E7%A7%91%E7%A0%94/">科研</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="MADRL总结"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="madrl"><a class="markdownIt-Anchor" href="#madrl"></a> MADRL</h1>
<div align='center'> <font size='20'><b>Deep Reinforcement Learning for Multiagent Systems: A Review of Challenges, Solutions, and Applications</b></font></div>
<div align='center'> <font size='20'>多Agent系统的深度强化学习：挑战、解决方案和应用综述</font></div>
<blockquote>
<table>
<thead>
<tr>
<th style="text-align:right">Key</th>
<th style="text-align:left">Value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right">文献类型</td>
<td style="text-align:left">journalArticle</td>
</tr>
<tr>
<td style="text-align:right">标题</td>
<td style="text-align:left">Deep Reinforcement Learning for Multiagent Systems: A Review of Challenges, Solutions, and Applications</td>
</tr>
<tr>
<td style="text-align:right">中文标题</td>
<td style="text-align:left">多Agent系统的深度强化学习：挑战、解决方案和应用综述</td>
</tr>
<tr>
<td style="text-align:right">作者</td>
<td style="text-align:left">[[Thanh Thi Nguyen]]、 [[Ngoc Duy Nguyen]]、 [[Saeid Nahavandi]]</td>
</tr>
<tr>
<td style="text-align:right">期刊名称</td>
<td style="text-align:left">[[IEEE Transactions on Cybernetics]]</td>
</tr>
<tr>
<td style="text-align:right">DOI</td>
<td style="text-align:left"><a target="_blank" rel="noopener" href="https://doi.org/10.1109/TCYB.2020.2977374">10.1109/TCYB.2020.2977374</a></td>
</tr>
<tr>
<td style="text-align:right">引用次数</td>
<td style="text-align:left">281 📊</td>
</tr>
<tr>
<td style="text-align:right">影响因子</td>
<td style="text-align:left">19.118 (Q1)</td>
</tr>
<tr>
<td style="text-align:right">条目链接</td>
<td style="text-align:left"><a href="zotero://select/groups/4830020/items/7GSXBWQ3" title="自用链接">My Library</a></td>
</tr>
<tr>
<td style="text-align:right">PDF 附件</td>
<td style="text-align:left"><a target="_blank" rel="noopener" href="https://www.jianguoyun.com/p/DbsZy6sQoO-BCxje5uAEIAA" title="访问密码 : 123456">2020__Nguyen et al__Deep Reinforcement Learning for Multiagent Systems.pdf</a></td>
</tr>
</tbody>
</table>
</blockquote>
<blockquote>
<h2 id="abstract"><a class="markdownIt-Anchor" href="#abstract"></a> Abstract</h2>
<p>Reinforcement learning (RL) algorithms have been around for decades and employed to solve various sequential decision-making problems. These algorithms, however, have faced great challenges when dealing with high-dimensional environments. The recent development of deep learning has enabled RL methods to drive optimal policies for sophisticated and capable agents, which can perform efficiently in these challenging environments. This article addresses an important aspect of deep RL related to situations that require multiple agents to communicate and cooperate to solve complex tasks. A survey of different approaches to problems related to multiagent deep RL (MADRL) is presented, including nonstationarity, partial observability, continuous state and action spaces, multiagent training schemes, and multiagent transfer learning. The merits and demerits of the reviewed methods will be analyzed and discussed with their corresponding applications explored. It is envisaged that this review provides insights about various MADRL methods and can lead to the future development of more robust and highly useful multiagent learning methods for solving real-world problems.</p>
<p>【摘要翻译】强化学习（RL）算法已经出现了几十年，并用于解决各种顺序决策问题。然而，这些算法在处理高维环境时面临着巨大的挑战。深度学习的最新发展使RL方法能够为复杂和有能力的代理驱动最优策略，这些代理可以在这些具有挑战性的环境中高效地执行。本文讨论了深度RL的一个重要方面，涉及到需要多个代理进行通信和协作以解决复杂任务的情况。综述了与多智能体深度RL（MADRL）相关问题的不同方法，包括非平稳性、部分可观测性、连续状态和动作空间、多智能体训练方案和多智能体转移学习。本文将分析和讨论这些方法的优缺点，并探讨它们的相应应用。据设想，这篇综述提供了关于各种MADRL方法的见解，并可导致未来发展更健壮、更有用的多智能体学习方法，用于解决现实问题。</p>
</blockquote>
<h2 id="single-agent-deep-rl"><a class="markdownIt-Anchor" href="#single-agent-deep-rl"></a> Single Agent Deep RL</h2>
<h3 id="actor-critic"><a class="markdownIt-Anchor" href="#actor-critic"></a> <strong>Actor-Critic</strong></h3>
<p>“The actor structure is used to select a suitable action according to the observed state and transfer to the critic structure for evaluation.”</p>
<p>行动者结构用于根据观察到的状态选择合适的行动，并转移到批评家结构进行评估。</p>
<h3 id="deep-q-network"><a class="markdownIt-Anchor" href="#deep-q-network"></a> “Deep Q-Network”</h3>
<p><strong>DQN</strong>使用Bellman公式来定义最小化损失函数：</p>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">L</mi><mo stretchy="false">(</mo><mi>β</mi><mo stretchy="false">)</mo><mo>=</mo><mi mathvariant="double-struck">E</mi><mrow><mo fence="true">[</mo><msup><mrow><mo fence="true">(</mo><mi>r</mi><mo>+</mo><mi>γ</mi><msub><mo><mi>max</mi><mo>⁡</mo></mo><msup><mi>a</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></msub><mi>Q</mi><mrow><mo fence="true">(</mo><msup><mi>s</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo separator="true">,</mo><msup><mi>a</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo>∣</mo><msup><mi>β</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo fence="true">)</mo></mrow><mo>−</mo><mi>Q</mi><mo stretchy="false">(</mo><mi>s</mi><mo separator="true">,</mo><mi>a</mi><mo>∣</mo><mi>β</mi><mo stretchy="false">)</mo><mo fence="true">)</mo></mrow><mn>2</mn></msup><mo fence="true">]</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{L}(\beta)=\mathbb{E}\left[\left(r+\gamma \max _{a^{\prime}} Q\left(s^{\prime}, a^{\prime} \mid \beta^{\prime}\right)-Q(s, a \mid \beta)\right)^2\right]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathcal">L</span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.05278em;">β</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.80002em;vertical-align:-0.65002em;"></span><span class="mord"><span class="mord mathbb">E</span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size2">[</span></span><span class="minner"><span class="minner"><span class="mopen delimcenter" style="top:0em;">(</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord mathnormal" style="margin-right:0.05556em;">γ</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop"><span class="mop">max</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32797999999999994em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">a</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6828285714285715em;"><span style="top:-2.786em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">Q</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">(</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.751892em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.751892em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∣</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05278em;">β</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.751892em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;">)</span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord mathnormal">Q</span><span class="mopen">(</span><span class="mord mathnormal">s</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">a</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∣</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord mathnormal" style="margin-right:0.05278em;">β</span><span class="mclose">)</span><span class="mclose delimcenter" style="top:0em;">)</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9559em;"><span style="top:-3.2047920000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size2">]</span></span></span></span></span></span></p>
<p>DQN的缺陷：</p>
<p>“However, using a neural network to approximate value function is proved to be unstable and may result in divergence due to the bias originated from correlative samples”</p>
<p>然而，使用神经网络逼近值函数被证明是不稳定的，并且由于样本相关性所产生的偏差可能导致<strong>发散</strong></p>
<p><strong>解决方案：</strong></p>
<p>设置一个<strong>experience replay memory</strong>来打断样本之间的关联</p>
<h3 id="double-dqn-ddqn-双dqn"><a class="markdownIt-Anchor" href="#double-dqn-ddqn-双dqn"></a> “Double DQN (DDQN)” 双DQN</h3>
<p>“The idea of DDQN is to separate the selection of “greedy” action from action evaluation.”</p>
<p>DDQN的思想是将“贪婪”行为的选择与行为评估分开。</p>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="script">L</mi><mrow><mi mathvariant="normal">D</mi><mi mathvariant="normal">D</mi><mi mathvariant="normal">Q</mi><mi mathvariant="normal">N</mi></mrow></msub><mo stretchy="false">(</mo><mi>β</mi><mo stretchy="false">)</mo><mo>=</mo><mi mathvariant="double-struck">E</mi><mrow><mo fence="true">[</mo><msup><mrow><mo fence="true">(</mo><mi>r</mi><mo>+</mo><mi>γ</mi><mi>Q</mi><mrow><mo fence="true">(</mo><msup><mi>s</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo separator="true">,</mo><mi>arg</mi><mo>⁡</mo><msub><mo><mi>max</mi><mo>⁡</mo></mo><msup><mi>a</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></msub><mi>Q</mi><mrow><mo fence="true">(</mo><msup><mi>s</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo separator="true">,</mo><msup><mi>a</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo>∣</mo><mi>β</mi><mo fence="true">)</mo></mrow><mo>∣</mo><msup><mi>β</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo fence="true">)</mo></mrow><mo>−</mo><mi>Q</mi><mo stretchy="false">(</mo><mi>s</mi><mo separator="true">,</mo><mi>a</mi><mo>∣</mo><mi>β</mi><mo stretchy="false">)</mo><mo fence="true">)</mo></mrow><mn>2</mn></msup><mo fence="true">]</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{L}_{\mathrm{DDQN}}(\beta)=\mathbb{E}\left[\left(r+\gamma Q\left(s^{\prime}, \arg \max _{a^{\prime}} Q\left(s^{\prime}, a^{\prime} \mid \beta\right) \mid \beta^{\prime}\right)-Q(s, a \mid \beta)\right)^2\right]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.036108em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord"><span class="mord mathcal">L</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.328331em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathrm mtight">D</span><span class="mord mathrm mtight">D</span><span class="mord mathrm mtight">Q</span><span class="mord mathrm mtight">N</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.05278em;">β</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.80002em;vertical-align:-0.65002em;"></span><span class="mord"><span class="mord mathbb">E</span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size2">[</span></span><span class="minner"><span class="minner"><span class="mopen delimcenter" style="top:0em;">(</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord mathnormal" style="margin-right:0.05556em;">γ</span><span class="mord mathnormal">Q</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">(</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.751892em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop">ar<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop"><span class="mop">max</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32797999999999994em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">a</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6828285714285715em;"><span style="top:-2.786em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">Q</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">(</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.751892em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.751892em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∣</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord mathnormal" style="margin-right:0.05278em;">β</span><span class="mclose delimcenter" style="top:0em;">)</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∣</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05278em;">β</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.751892em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;">)</span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord mathnormal">Q</span><span class="mopen">(</span><span class="mord mathnormal">s</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">a</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∣</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord mathnormal" style="margin-right:0.05278em;">β</span><span class="mclose">)</span><span class="mclose delimcenter" style="top:0em;">)</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9559em;"><span style="top:-3.2047920000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size2">]</span></span></span></span></span></span></p>
<p>关键区别在于<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.05278em;">β</span></span></span></span> 在公式里面是不带撇的，意思是使用当前的参数来估计一个最大<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>a</mi><mi mathvariant="normal">‘</mi></msup></mrow><annotation encoding="application/x-tex">a^`</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.849108em;vertical-align:0em;"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">‘</span></span></span></span></span></span></span></span></span></span></span>  得到的最大Q值。</p>
<p>问题：</p>
<p>“However, the fact that selecting randomly samples from the experience replay does not completely separate the sample data”</p>
<p>“然而，从经验回放中随机选择样本并不能完全分离样本数据”</p>
<p>方案：prioritized experience replay</p>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mi>i</mi></msub><mo>=</mo><mrow><mo fence="true">∣</mo><msub><mi>δ</mi><mi>i</mi></msub><mo fence="true">∣</mo></mrow><mo>=</mo><mrow><mo fence="true">∣</mo><msub><mi>r</mi><mi>i</mi></msub><mo>+</mo><mi>γ</mi><msub><mo><mi>max</mi><mo>⁡</mo></mo><mi>a</mi></msub><mi>Q</mi><mrow><mo fence="true">(</mo><msub><mi>s</mi><mi>i</mi></msub><mo separator="true">,</mo><mi>a</mi><mo>∣</mo><msup><mi>β</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo fence="true">)</mo></mrow><mo>−</mo><mi>Q</mi><mrow><mo fence="true">(</mo><msub><mi>s</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mo separator="true">,</mo><msub><mi>a</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>∣</mo><mi>β</mi><mo fence="true">)</mo></mrow><mo fence="true">∣</mo></mrow></mrow><annotation encoding="application/x-tex">p_i=\left|\delta_i\right|=\left|r_i+\gamma \max _a Q\left(s_i, a \mid \beta^{\prime}\right)-Q\left(s_{i-1}, a_{i-1} \mid \beta\right)\right|</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">∣</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03785em;">δ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.03785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;">∣</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.001892em;vertical-align:-0.25em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">∣</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord mathnormal" style="margin-right:0.05556em;">γ</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop"><span class="mop">max</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">a</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">Q</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">(</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">a</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∣</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05278em;">β</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.751892em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;">)</span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord mathnormal">Q</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">(</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∣</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord mathnormal" style="margin-right:0.05278em;">β</span><span class="mclose delimcenter" style="top:0em;">)</span></span><span class="mclose delimcenter" style="top:0em;">∣</span></span></span></span></span></p>
<p>采用Q值之间的绝对TD差作为样本的随机抽取优先级。</p>
<h3 id="dueling-network-决斗网络"><a class="markdownIt-Anchor" href="#dueling-network-决斗网络"></a> <strong>“Dueling network”</strong> <strong>决斗网络</strong></h3>
<p>DQN策略选取会遇到一个问题就是又是会产生多个候选情况，而这两者都看起来没有负面作用，但是在之后的过程中其中某些会产生问题。作者在这里用了一个比喻：</p>
<p>“when driving a car and there are no obstables ahead, we can follow either the left lane or the right lane. If there is an obstacle ahead in the left lane, we must be in the right lane to avoid crashing. Therefore, it is more efficient if we focus only on the road and obstacles ahead.”</p>
<p>当开车时，前面没有障碍物，我们可以走左车道或右车道。如果前面的左车道有障碍物，我们必须在右车道以避免撞车。因此，如果我们只关注前面的道路和障碍，效率会更高。</p>
<p>“In the dueling architecture, there are two collateral networks that coexist: one network, parameterized by θ , estimates the state-value function V(s|θ) and the other one, parameterized by θ ′, estimates the advantage action function A(s, a|θ ′).”</p>
<p>决斗网络就是采用状态-值函数V和优势函数A来共同构成Q函数，这样不但使用了状态值，还对不同的状态-动作进行了优势评估，综合决定Q函数。</p>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable rowspacing="0.24999999999999992em" columnalign="right left" columnspacing="0em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mi>Q</mi><mrow><mo fence="true">(</mo><mi>s</mi><mo separator="true">,</mo><mi>a</mi><mo>∣</mo><mi>θ</mi><mo separator="true">,</mo><msup><mi>θ</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo fence="true">)</mo></mrow><mo>=</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mi>V</mi><mo stretchy="false">(</mo><mi>s</mi><mo>∣</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>+</mo><mrow><mo fence="true">(</mo><mi>A</mi><mrow><mo fence="true">(</mo><mi>s</mi><mo separator="true">,</mo><mi>a</mi><mo>∣</mo><msup><mi>θ</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo fence="true">)</mo></mrow><mo>−</mo><mfrac><mn>1</mn><mrow><mo fence="true">∣</mo><msub><mi mathvariant="normal">Δ</mi><mi>π</mi></msub><mo fence="true">∣</mo></mrow></mfrac><munder><mo>∑</mo><msup><mi>a</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></munder><mi>A</mi><mrow><mo fence="true">(</mo><mi>s</mi><mo separator="true">,</mo><msup><mi>a</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo>∣</mo><msup><mi>θ</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo fence="true">)</mo></mrow><mo fence="true">)</mo></mrow></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned} Q\left(s, a \mid \theta, \theta^{\prime}\right)=&amp; V(s \mid \theta) \\ &amp;+\left(A\left(s, a \mid \theta^{\prime}\right)-\frac{1}{\left|\Delta_\pi\right|} \sum_{a^{\prime}} A\left(s, a^{\prime} \mid \theta^{\prime}\right)\right) \end{aligned}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:4.843985em;vertical-align:-2.1719925em;"></span><span class="mord"><span class="mtable"><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.6719925em;"><span style="top:-5.5819925em;"><span class="pstrut" style="height:3.75em;"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">(</span><span class="mord mathnormal">s</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">a</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∣</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.801892em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;">)</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span></span></span><span style="top:-3.1719925em;"><span class="pstrut" style="height:3.75em;"></span><span class="mord"></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:2.1719925em;"><span></span></span></span></span></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.6719925em;"><span style="top:-5.5819925em;"><span class="pstrut" style="height:3.75em;"></span><span class="mord"><span class="mord"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mopen">(</span><span class="mord mathnormal">s</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∣</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="mclose">)</span></span></span><span style="top:-3.1719925em;"><span class="pstrut" style="height:3.75em;"></span><span class="mord"><span class="mord"></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size4">(</span></span><span class="mord mathnormal">A</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">(</span><span class="mord mathnormal">s</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">a</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∣</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.801892em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;">)</span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.32144em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="minner"><span class="mopen delimcenter" style="top:0em;">∣</span><span class="mord"><span class="mord">Δ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">π</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;">∣</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.936em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.050005em;"><span style="top:-1.8560149999999997em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">a</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6828285714285715em;"><span style="top:-2.786em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.0500049999999996em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.2939850000000002em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">A</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">(</span><span class="mord mathnormal">s</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.801892em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∣</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.801892em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;">)</span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size4">)</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:2.1719925em;"><span></span></span></span></span></span></span></span></span></span></span></p>
<h3 id="deep-recurrent-q-network-drqn-深度递归q网络"><a class="markdownIt-Anchor" href="#deep-recurrent-q-network-drqn-深度递归q网络"></a> “Deep recurrent Q-network (DRQN)” 深度递归Q网络</h3>
<p>“These games are often called partially observable MDP problems. The straightforward solution is to replace the fully connected layer right after the last convolutional layer of the policy network with a recurrent long shortterm memory,”</p>
<p>DQN的另一个缺点是它使用四个帧的历史作为策略网络的输入。因此，DQN在解决当前状态依赖于大量历史信息（如“双灌篮”或“冻伤”游戏）的问题时效率低下，这些游戏通常被称为部分可观察的MDP问题。直截了当的解决方案是将策略网络最后一个卷积层之后的完全连接层替换为经常性的<strong>长短期记忆</strong></p>
<h3 id="deep-attention-recurrent-q-network-darqn-深度关注复发q网络darqn"><a class="markdownIt-Anchor" href="#deep-attention-recurrent-q-network-darqn-深度关注复发q网络darqn"></a> “Deep attention recurrent Q-network (DARQN)” 深度关注复发Q网络（DARQN）</h3>
<p>“Sorokin et al. added attention mechanism into DRQN so that the network can focus only on important regions in the game, allowing smaller network’s parameters and hence speeding the training process.”</p>
<p>“Sorokin等人在DRQN中添加了注意力机制，这样网络就可以只关注游戏中的重要区域，从而允许较小的网络参数，从而加快训练过程。</p>
<h2 id="multi-agent-deep-rl"><a class="markdownIt-Anchor" href="#multi-agent-deep-rl"></a> Multi-agent Deep RL</h2>
<h3 id="nonstationarity-非平稳性"><a class="markdownIt-Anchor" href="#nonstationarity-非平稳性"></a> “Nonstationarity” :非平稳性</h3>
<p>“Learning among the agents is complex because all agents potentially interact with each other and learn concurrently. The interactions among multiple agents constantly reshape the environment and lead to nonstationarity.”</p>
<p>智能体之间的学习很复杂，因为所有智能体都可能相互交互并同时学习。多个智能体之间的交互不断重塑环境并导致非平稳性（nonstationarity）的产生。</p>
<p>在这种情况下，智能体之间的学习有时会导致代理的策略发生变化，并可能影响其他智能体的最优策略。对一个动作的潜在回报的估计是不准确的，因此，在多智能体环境中的给定点上，好的政策在未来不可能保持不变。</p>
<p>“The convergence theory of Q-learning applied in a single-agent setting is not guaranteed to most multiagent problems as the Markov property does not hold anymore in the nonstationary environment”</p>
<p>由于马尔可夫特性在非平稳环境中不再成立，因此在单智能体环境中应用的Q学习收敛理论不能保证适用于大多数多智能体问题.</p>
<h4 id="deep-repeated-update-q-network-druqn-深度重复更新q网络"><a class="markdownIt-Anchor" href="#deep-repeated-update-q-network-druqn-深度重复更新q网络"></a> “deep repeated update Q-network (DRUQN)” 深度重复更新Q网络</h4>
<h4 id="deep-loosely-coupled-q-network-dlcqn-深度松耦合q网络"><a class="markdownIt-Anchor" href="#deep-loosely-coupled-q-network-dlcqn-深度松耦合q网络"></a> “deep loosely coupled Q-network (DLCQN)” 深度松耦合Q网络</h4>
<p>“DLCQN relies on the loosely coupled Q-learning proposed in [52], which specifies and adjusts an independence degree for each agent using its negative rewards and observations. Through this independence degree, the agent learns to decide whether it needs to act independently or cooperate with other agents in different circumstances.”</p>
<p>DLCQN依赖于[52]中提出的松散耦合Q-学习，该学习算法使用其负面奖励和观察结果为每个智能体指定和调整独立度。通过这种独立程度，智能体学会在不同的情况下决定是否需要独立行动或与其他智能体合作。</p>
<h4 id="lenient-dqn-ldqn-仁慈dqnldqn"><a class="markdownIt-Anchor" href="#lenient-dqn-ldqn-仁慈dqnldqn"></a> “lenient-DQN (LDQN)”  仁慈DQN（LDQN）</h4>
<p>“Leniency in the context of a multiagent setting describes the situation where a learning agent ignores the poor actions of a co-learner, which leads to low rewards, and still cooperates with the co-learner with the hope that the co-learner can improve his actions in the future.”</p>
<p>在多智能体环境中，宽容是指learning智能体忽略了合作学习者的不良行为，从而导致低回报，但是仍然与合作学习者合作，希望合作学习者能够在未来改进其行为的情况。</p>
<h4 id="hystereticdqn-hdqn-滞后dqnhdqn"><a class="markdownIt-Anchor" href="#hystereticdqn-hdqn-滞后dqnhdqn"></a> “hystereticDQN (HDQN)” 滞后DQN（HDQN）</h4>
<p>“The experimental results demonstrate the superiority of LDQN against HDQN in terms of convergence to optimal policies in a stochastic reward environment.”</p>
<h4 id="weighted-ddqn-加权ddqn"><a class="markdownIt-Anchor" href="#weighted-ddqn-加权ddqn"></a> “weighted DDQN” 加权DDQN</h4>
<p>“The experiments show the better performance of WDDQN against the DDQN in two multiagent environments with stochastic rewards and large state space.”</p>
<p>实验表明，WDDQN在两个具有随机奖励和大状态空间的多智能体环境中对DDQN具有更好的性能。</p>
<p>LDQN&gt;HDQN</p>
<p>WDDQN&gt;DDQN</p>
<h3 id="partial-observabilitypmod"><a class="markdownIt-Anchor" href="#partial-observabilitypmod"></a> “Partial Observability:”PMOD</h3>
<h4 id="drqn"><a class="markdownIt-Anchor" href="#drqn"></a> DRQN</h4>
<p>“Unlike DQN, DRQN approximates Q(o, a), which is a Q-function with an observation o and an action a, by a recurrent neural network. DRQN treats a hidden state of the network <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mrow><mi>t</mi><mtext>−</mtext><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">h_{t−1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.902771em;vertical-align:-0.208331em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mord mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span></span></span></span> as an internal state.”</p>
<p>与DQN不同，DRQN通过递归神经网络近似于Q（o，a），这是一个具有观测值o和动作a的Q函数。DRQN处理网络的隐藏状态ht−1作为内部状态。</p>
<p>“With the recurrent structure, the DRQN-based agents are able to learn the improved policy in a robust sense in the partially observable environment”</p>
<h4 id="deep-distributed-recurrent-q-network-ddrqn-深分布递归q网络"><a class="markdownIt-Anchor" href="#deep-distributed-recurrent-q-network-ddrqn-深分布递归q网络"></a> “Deep distributed recurrent Q-network (DDRQN)” 深分布递归Q网络</h4>
<p>“The success of DDRQN is relied on three notable features, that is, last-action inputs, interagent weight sharing, and disabling experience replay”</p>
<ul>
<li>
<h4 id="last-action-inputs"><a class="markdownIt-Anchor" href="#last-action-inputs"></a> last-action inputs:</h4>
</li>
</ul>
<p>“last-action inputs, requires the provision of the previous action of each agent as input to its next step”</p>
<ul>
<li>
<h4 id="interagent-weight-sharing"><a class="markdownIt-Anchor" href="#interagent-weight-sharing"></a> interagent weight sharing:</h4>
</li>
</ul>
<p>“The interagent weight sharing means that all agents use weights of only one network, which is learned during the training process.”</p>
<ul>
<li>
<h4 id="disabling-experience-replay"><a class="markdownIt-Anchor" href="#disabling-experience-replay"></a> disabling experience replay:</h4>
</li>
</ul>
<p>“The disabling experience replay simply excludes the experience replay feature of DQN”</p>
<p>“The DDRQN therefore learns a Q-function of the form <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi><mo stretchy="false">(</mo><msubsup><mi>o</mi><mi>m</mi><mi>t</mi></msubsup><mo separator="true">,</mo><msubsup><mi>h</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mi>m</mi></msubsup><mo separator="true">,</mo><mi>m</mi><mo separator="true">,</mo><msubsup><mi>a</mi><mrow><mi>t</mi><mtext>−</mtext><mn>1</mn></mrow><mi>m</mi></msubsup><mo separator="true">,</mo><msubsup><mi>a</mi><mi>t</mi><mi>m</mi></msubsup><mo separator="true">;</mo><mi>θ</mi><mi>i</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">Q(o^t_m, h^m_{t-1}, m, a^m_{t−1}, a^m_t; θi)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0999949999999998em;vertical-align:-0.30643899999999996em;"></span><span class="mord mathnormal">Q</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">o</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7935559999999999em;"><span style="top:-2.4530000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">m</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6643919999999999em;"><span style="top:-2.451892em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.30643899999999996em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">m</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6643919999999999em;"><span style="top:-2.451892em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mord mtight">−</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.30643899999999996em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.664392em;"><span style="top:-2.4530000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="mord mathnormal">i</span><span class="mclose">)</span></span></span></span>, where each agent receives its own index m as the input. Weight sharing decreases learning time because it reduces the number of parameters to be learned.”</p>
<p>因此，DDRQN学习<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi><mo stretchy="false">(</mo><msubsup><mi>o</mi><mi>m</mi><mi>t</mi></msubsup><mo separator="true">,</mo><msubsup><mi>h</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mi>m</mi></msubsup><mo separator="true">,</mo><mi>m</mi><mo separator="true">,</mo><msubsup><mi>a</mi><mrow><mi>t</mi><mtext>−</mtext><mn>1</mn></mrow><mi>m</mi></msubsup><mo separator="true">,</mo><msubsup><mi>a</mi><mi>t</mi><mi>m</mi></msubsup><mo separator="true">;</mo><mi>θ</mi><mi>i</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">Q(o^t_m, h^m_{t-1}, m, a^m_{t−1}, a^m_t; θi)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0999949999999998em;vertical-align:-0.30643899999999996em;"></span><span class="mord mathnormal">Q</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">o</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7935559999999999em;"><span style="top:-2.4530000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">m</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6643919999999999em;"><span style="top:-2.451892em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.30643899999999996em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">m</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6643919999999999em;"><span style="top:-2.451892em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mord mtight">−</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.30643899999999996em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.664392em;"><span style="top:-2.4530000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="mord mathnormal">i</span><span class="mclose">)</span></span></span></span>形式的Q函数，其中每个智能体接收自己的索引m作为输入。权重共享减少了学习时间，因为它减少了要学习的参数的数量。</p>
<p>问题：</p>
<p>“Although each agent has a different observation and hidden state, this approach, however, assumes that agents have the same set of actions.”</p>
<p>DDRQN假设所有的agent都有相同的动作集，这在现实中某些复杂场景中是不太适用的。</p>
<h4 id="deep-policy-inference-q-network-dpiqn-深度策略推理q网络"><a class="markdownIt-Anchor" href="#deep-policy-inference-q-network-dpiqn-深度策略推理q网络"></a> “Deep policy inference Q-network (DPIQN)” 深度策略推理Q网络</h4>
<h4 id="eep-recurrent-policy-inference-q-network-drpiqn-深度循环策略推理q网络drpiqn"><a class="markdownIt-Anchor" href="#eep-recurrent-policy-inference-q-network-drpiqn-深度循环策略推理q网络drpiqn"></a> “eep recurrent policy inference Q-network (DRPIQN)” 深度循环策略推理Q网络（DRPIQN）</h4>
<p>“Both DPIQN and DRPIQN are learned by adapting the network’s attention to policy features and their own Q-values at various stages of the training process.”</p>
<h4 id="multitask-marl-mt-marl-多任务marlmt-marl"><a class="markdownIt-Anchor" href="#multitask-marl-mt-marl-多任务marlmt-marl"></a> “Multitask MARL (MT-MARL)” 多任务MARL（MT-MARL）</h4>
<p>“multitask MARL (MT-MARL) that integrates hysteretic learners [62], DRQNs [44], distillation [63], and concurrent experience replay trajectories (CERTs),”</p>
<p>“which are a decentralized extension of experience replay strategy proposed in [6]. The agents are not explicitly provided with task identity (thus partial observability) while they cooperatively learn to complete a set of decentralized POMDP tasks with sparse rewards.”这是[6]中提出的经验重放策略的分散扩展。当智能体协作学习完成一组分散的POMDP任务时，没有明确地向其提供任务身份（因此是部分可观察性），并且回报很少。</p>
<h4 id="maddpg-m"><a class="markdownIt-Anchor" href="#maddpg-m"></a> “MADDPG-M”</h4>
<h4 id="deep-deterministic-policy-gradient-ddpg-深度确定性策略梯度"><a class="markdownIt-Anchor" href="#deep-deterministic-policy-gradient-ddpg-深度确定性策略梯度"></a> “Deep deterministic policy gradient (DDPG)” 深度确定性策略梯度</h4>
<p>“Agents need to decide whether their observations are informative to share with other agents and the communication policies are learned concurrently with the main policies through experience”</p>
<h4 id="bayesian-action-decoder-bad-贝叶斯动作解码器bad"><a class="markdownIt-Anchor" href="#bayesian-action-decoder-bad-贝叶斯动作解码器bad"></a> “Bayesian action decoder (BAD)” 贝叶斯动作解码器（BAD）</h4>
<p>“BAD relies on a factorized and approximate belief state to discover conventions to enable agents to learn optimal policies efficiently. This is closely relevant to theory of mind that humans normally use to interpret others’ actions.”</p>
<p>BAD依赖于一个分解的近似信念状态来发现约定，从而使代理能够有效地学习最优策略。这与人类通常用来解释他人行为的心理理论密切相关。</p>
<p>“The experimental results on a proof-of-principle two-step matrix game and the cooperative partial-information card game Hanabi demonstrate the efficiency and superiority of the proposed method against traditional policy gradient algorithms”</p>
<p>“原理证明两步矩阵游戏和合作部分信息卡游戏Hanabi的实验结果表明，相对于传统的策略梯度算法，该方法具有效率和优越性”</p>
<h3 id="mas-training-schemes"><a class="markdownIt-Anchor" href="#mas-training-schemes"></a> “MAS Training Schemes”</h3>
<h4 id="clde-centralized-learning-and-decentralized-execution-集中学习和分散执行"><a class="markdownIt-Anchor" href="#clde-centralized-learning-and-decentralized-execution-集中学习和分散执行"></a> “CLDE Centralized Learning and Decentralized Execution” 集中学习和分散执行</h4>
<h5 id="centralized-learning"><a class="markdownIt-Anchor" href="#centralized-learning"></a> <strong>centralized learning</strong>：</h5>
<p>where a group of agents can be trained simultaneously by applying a centralized method via an open communication channel</p>
<h5 id="decentralized-execution"><a class="markdownIt-Anchor" href="#decentralized-execution"></a> <strong>decentralized execution</strong>:</h5>
<p>Decentralized policies where each agent can take actions based on its local observations have an advantage under partial observability and in limited communications during execution.</p>
<h4 id="centralized-learning-2"><a class="markdownIt-Anchor" href="#centralized-learning-2"></a> centralized learning</h4>
<h4 id="concurrent-learning-并行学习"><a class="markdownIt-Anchor" href="#concurrent-learning-并行学习"></a> concurrent learning 并行学习</h4>
<p>并行学习是从自己的观察进行学习，而集中学习是从共同观察经验中进行学习。</p>
<h4 id="centralized-learning-和-concurrent-learning-的区别"><a class="markdownIt-Anchor" href="#centralized-learning-和-concurrent-learning-的区别"></a> centralized learning 和 concurrent learning 的区别：</h4>
<p>“Centralized policy attempts to obtain a joint action from joint observations of all agents while the concurrent learning trains agents simultaneously using the joint reward signal.”</p>
<p>集中式策略试图从所有代理的联合观察中获得联合行动，同时并行学习使用联合奖励信号同时训练智能体。</p>
<h4 id="parameter-sharing-参数共享"><a class="markdownIt-Anchor" href="#parameter-sharing-参数共享"></a> “parameter sharing” 参数共享</h4>
<p>“scheme allows agents to be trained simultaneously using the experiences of all agents although each agent can obtain unique observations.”</p>
<p>即便每一个智能体都有自己独立的观察，但是参数共享可以同时对多个智能体进行训练，因此可以将单个智能体的深度学习拓展到多智能体</p>
<h4 id="parameter-sharing-and-trpo-参数共享和trpo"><a class="markdownIt-Anchor" href="#parameter-sharing-and-trpo-参数共享和trpo"></a> “Parameter Sharing and TRPO” 参数共享和TRPO</h4>
<h4 id="reinforced-interagent-learning-rial-强化交互学习rial"><a class="markdownIt-Anchor" href="#reinforced-interagent-learning-rial-强化交互学习rial"></a> “Reinforced interagent learning (RIAL)” 强化交互学习（RIAL）</h4>
<p>“deep Q-learning has a recurrent structure to address the partial observability issue, in which independent Q-learning offers individual agents to learn their own network parameters.”</p>
<h4 id="differentiable-interagent-learning-dial-可微分交互学习dial"><a class="markdownIt-Anchor" href="#differentiable-interagent-learning-dial-可微分交互学习dial"></a> “Differentiable interagent learning (DIAL)” 可微分交互学习（DIAL）</h4>
<p>“DIAL pushes gradients from one agent to another through a channel, allowing end-to-end backpropagation across agents”</p>
<h4 id="deep-reinforcement-opponent-network-dron-深加固对手网络"><a class="markdownIt-Anchor" href="#deep-reinforcement-opponent-network-dron-深加固对手网络"></a> “Deep reinforcement opponent network (DRON)” 深加固对手网络</h4>
<p>“encodes observation of the opponent agent into DQN to jointly learn a policy and behaviors of opponents without domain knowledge”将对手智能体的观察结果编码到DQN中，以共同学习对手的策略和行为，而无需领域知识</p>
<h4 id="multiagent-deep-deterministic-policy-gradient-maddpg-多代理深度确定性策略梯度maddpg"><a class="markdownIt-Anchor" href="#multiagent-deep-deterministic-policy-gradient-maddpg-多代理深度确定性策略梯度maddpg"></a> “Multiagent deep deterministic policy gradient (MADDPG)” 多代理深度确定性策略梯度（MADDPG）</h4>
<p>“MADDPG features the centralized learning and decentralized execution paradigm in which the critic uses extra information to ease the training process while actors take actions based on their own local observations.”</p>
<h4 id="counterfactual-multiagent-coma-反事实多智能体"><a class="markdownIt-Anchor" href="#counterfactual-multiagent-coma-反事实多智能体"></a> “Counterfactual multiagent (COMA)” 反事实多智能体</h4>
<p>“Unlike MADDPG [74], COMA can handle the multiagent credit assignment problem [76] where agents are difficult to work out their contribution to the team’s success from global rewards generated by joint actions in cooperative settings. COMA, however, has a disadvantage that focuses only on discrete action space while MADDPG is able to learn continuous policies effectively.”</p>
<p>与MADDPG不同，COMA可以处理多代理人信贷分配问题[76]，代理人很难从合作环境中的联合行动产生的全球奖励中计算出他们对团队成功的贡献。然而，COMA有一个缺点，即只关注离散的行动空间，而MADDPG能够有效地学习连续策略。</p>
<h3 id="continuous-action-spaces-连续动作空间"><a class="markdownIt-Anchor" href="#continuous-action-spaces-连续动作空间"></a> “Continuous Action Spaces” 连续动作空间</h3>
<p>DQN 的方法局限于有限空间动作空间内寻求解，即DQN旨在寻找具有最大动作值的动作，因此需要在连续动作（状态）空间的每一步进行迭代优化过程，将动作空间离散化是一个方法，但是这样会产生很多问题，比如纬度诅咒问题，动作数相对于自由度的指数级增长。</p>
<h4 id="trust-region-policy-optimization-trpo-信任区域策略优化"><a class="markdownIt-Anchor" href="#trust-region-policy-optimization-trpo-信任区域策略优化"></a> “Trust region policy optimization (TRPO)” 信任区域策略优化</h4>
<h4 id="deterministic-policy-gradient-dpg-确定性策略梯度"><a class="markdownIt-Anchor" href="#deterministic-policy-gradient-dpg-确定性策略梯度"></a> “Deterministic policy gradient (DPG)” 确定性策略梯度</h4>
<h4 id="recurrent-dpg-rdpg-经常性dpgrdpg"><a class="markdownIt-Anchor" href="#recurrent-dpg-rdpg-经常性dpgrdpg"></a> “Recurrent DPG (RDPG)” 经常性DPG（RDPG）</h4>
<h4 id="ps-trpo"><a class="markdownIt-Anchor" href="#ps-trpo"></a> “PS-TRPO”</h4>
<p>“This method is based on the foundation of TRPO so that it can deal with continuous action spaces effectively.”</p>
<h3 id="transfer-learning-for-madrl-madrl的迁移学习"><a class="markdownIt-Anchor" href="#transfer-learning-for-madrl-madrl的迁移学习"></a> “Transfer Learning for MADRL” MADRL的迁移学习</h3>
<h4 id="policy-distillation-政策蒸馏"><a class="markdownIt-Anchor" href="#policy-distillation-政策蒸馏"></a> “Policy distillation” 政策蒸馏</h4>
<h4 id="progressive-neural-networks-渐进式神经网络"><a class="markdownIt-Anchor" href="#progressive-neural-networks-渐进式神经网络"></a> “Progressive neural networks” 渐进式神经网络</h4>
<h4 id="actor-mimic-method-演员模拟法"><a class="markdownIt-Anchor" href="#actor-mimic-method-演员模拟法"></a> “Actor-mimic method” 演员模拟法</h4>
<h3 id="madrl-applications"><a class="markdownIt-Anchor" href="#madrl-applications"></a> MADRL Applications</h3>
<h4 id="sequential-social-dilemma-ssd-model-顺序社会困境ssd模型"><a class="markdownIt-Anchor" href="#sequential-social-dilemma-ssd-model-顺序社会困境ssd模型"></a> “Sequential social dilemma (SSD) model” 顺序社会困境（SSD）模型</h4>
<h4 id="extension-of-matrix-game-social-dilemma-mgsd-矩阵博弈社会困境的扩展"><a class="markdownIt-Anchor" href="#extension-of-matrix-game-social-dilemma-mgsd-矩阵博弈社会困境的扩展"></a> “Extension of matrix game social dilemma (MGSD)” 矩阵博弈社会困境的扩展</h4>
<h4 id="swarm-systems-群系统"><a class="markdownIt-Anchor" href="#swarm-systems-群系统"></a> “swarm systems” 群系统</h4>
<h4 id="dueling-ddqn-dddqn-dueling-ddqndddqn"><a class="markdownIt-Anchor" href="#dueling-ddqn-dddqn-dueling-ddqndddqn"></a> “Dueling DDQN (DDDQN)” Dueling DDQN（DDDQN）</h4>
<h4 id="asynchronous-advantage-ac-a3c-异步优势aca3c"><a class="markdownIt-Anchor" href="#asynchronous-advantage-ac-a3c-异步优势aca3c"></a> “Asynchronous advantage AC (A3C)” 异步优势AC（A3C）</h4>
<h4 id="commnet-model-commnet模型"><a class="markdownIt-Anchor" href="#commnet-model-commnet模型"></a> “CommNet model” CommNet模型</h4>
<h4 id="contextual-deep-q-learning-上下文深层q-学习"><a class="markdownIt-Anchor" href="#contextual-deep-q-learning-上下文深层q-学习"></a> “Contextual deep Q-learning” 上下文深层Q-学习</h4>
<h4 id="contextual-multiagent-ac-上下文多智能体ac"><a class="markdownIt-Anchor" href="#contextual-multiagent-ac-上下文多智能体ac"></a> “Contextual multiagent AC” 上下文多智能体AC</h4>
<h2 id="future-challenges-research-directions"><a class="markdownIt-Anchor" href="#future-challenges-research-directions"></a> Future Challenges &amp; Research Directions</h2>
<h3 id="imitation-learning-模仿学习"><a class="markdownIt-Anchor" href="#imitation-learning-模仿学习"></a> “Imitation learning” 模仿学习</h3>
<p>“imitation learning tries to map between states to actions as a supervised approach. It directly generalizes the expert strategy to unvisited states so that it is close to a multiclass classification problem in cases of finite action set.”<br />
模仿学习作为一种有监督的方法，试图在状态和行为之间进行映射。它将专家策略直接推广到未访问的状态，也因此模仿学习较为接近于有限动作集情况下的<strong>多类分类问题</strong>。</p>
<h3 id="inverse-rl-反向rl"><a class="markdownIt-Anchor" href="#inverse-rl-反向rl"></a> “Inverse RL” 反向RL</h3>
<p>“Inverse RL assumes that the expert policy is optimal regarding the unknown reward function”<br />
反向强化学习假设关于未知回报函数的专家策略是最优的</p>
<p>“A very straightforward challenge arose from these applications is the requirement of multiple experts who are able to demonstrate the tasks collaboratively.”</p>
<p>这些应用程序产生的一个非常直接的挑战是需要多个能够协作演示任务的专家。</p>
<p>“Furthermore, the communication and reasoning capabilities of experts are difficult to be characterized and modeled by autonomous agents in the MAS domain.”</p>
<p>此外，在MAS领域中，专家的通信和推理能力很难被自主智能体识别和建模。</p>
<p>“In addition, for complex tasks or behaviors which are difficult for humans to demonstrate, there is a need for alternative methods that allow human preferences to be integrated into deep RL”</p>
<p>此外，对于人类难以演示的复杂任务或行为，需要替代方法，将人类偏好融入到深度RL中。</p>
<h3 id="human-on-the-loop"><a class="markdownIt-Anchor" href="#human-on-the-loop"></a> “Human-on-the-loop”</h3>
<p>“In human-on-the-loop, agents execute their tasks autonomously until completion, with a human in a monitoring or supervisory role reserving the ability to intervene in operations carried out by agents.”</p>
<p>即人类作为监督者，由机器完全自主完成任务，人类只作为监督管理，保留干预其执行任务的能力</p>
<h3 id="scaling-to-large-systems"><a class="markdownIt-Anchor" href="#scaling-to-large-systems"></a> “Scaling to large systems”</h3>
<h4 id="homogeneous-agents-同质多智能体"><a class="markdownIt-Anchor" href="#homogeneous-agents-同质多智能体"></a> Homogeneous agents 同质多智能体</h4>
<p>“Since agents have common behaviors, such as actions, domain knowledge, and goals (homogeneous agents), the scalability can be achievable by (partially) centralized training and decentralized execution”</p>
<h4 id="heterogeneous-agents-异质多智能体"><a class="markdownIt-Anchor" href="#heterogeneous-agents-异质多智能体"></a> Heterogeneous agents 异质多智能体</h4>
<p>“In the heterogeneous setting with many agents, the key challenge is how to provide the most optimal solution and maximize the task completion success based on self-learning with effective coordinative and cooperative strategies among the agents.”</p>
<p>方向、挑战总结：</p>
<ol>
<li>将专家经验进行分类和归纳总结，让自主智能体能够识别并进行建模</li>
<li>将人类的互动和偏好和深度学习相结合</li>
<li>深度学习很难和人类现有的人机协作科技产生互动，人类很难处理大量的单调工作，而机器很难处理创造性工作</li>
</ol>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://image-1302790708.cos.ap-chengdu.myqcloud.com/202210262141234.svg" alt="MADRL" title="思维导图" /></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="http://www.zhengxiaodong.com">郑晓东</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="http://www.zhengxiaodong.com/2022/10/26/MADRL%E6%80%BB%E7%BB%93/">http://www.zhengxiaodong.com/2022/10/26/MADRL总结/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/MADRL/">MADRL</a><a class="post-meta__tags" href="/tags/RL/">RL</a><a class="post-meta__tags" href="/tags/%E7%BB%BC%E8%BF%B0/">综述</a></div><div class="post_share"><div class="social-share" data-image="https://image-1302790708.cos.ap-chengdu.myqcloud.com/20221106221145.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i> Donate</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/pics/wechat.jpg" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/pics/wechat.jpg" alt="wechat"/></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="/pics/alipay.jpg" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/pics/alipay.jpg" alt="alipay"/></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2022/10/28/%E5%88%B7%E6%96%B0cdn/"><img class="prev-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://image-1302790708.cos.ap-chengdu.myqcloud.com/20221106221031.png" onerror="onerror=null;src='/pics/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info">刷新cdn</div></div></a></div><div class="next-post pull-right"><a href="/2022/10/26/Power-method-%E4%BC%B0%E8%AE%A1%E5%A4%8D%E6%9D%82%E5%BA%A6/"><img class="next-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://image-1302790708.cos.ap-chengdu.myqcloud.com/research.png" onerror="onerror=null;src='/pics/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">Power method 估计复杂度</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><div><a href="/2022/10/26/Alpha-alpha-Rank/" title="Alpha-alpha-Rank"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://image-1302790708.cos.ap-chengdu.myqcloud.com/20221106221319.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-10-26</div><div class="title">Alpha-alpha-Rank</div></div></a></div><div><a href="/2022/10/26/Log-Barrier-method/" title="Log-Barrier-method"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://image-1302790708.cos.ap-chengdu.myqcloud.com/research.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-10-26</div><div class="title">Log-Barrier-method</div></div></a></div><div><a href="/2022/11/01/Fast-Reinforcement-Learning-I/" title="cs234-11: Fast Reinforcement Learning I"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://image-1302790708.cos.ap-chengdu.myqcloud.com/20221106220939.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-11-01</div><div class="title">cs234-11: Fast Reinforcement Learning I</div></div></a></div><div><a href="/2022/11/20/On-Policy-Approximation/" title="On Policy Approximation"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://image-1302790708.cos.ap-chengdu.myqcloud.com/research.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-11-20</div><div class="title">On Policy Approximation</div></div></a></div><div><a href="/2022/10/26/Power-method-%E4%BC%B0%E8%AE%A1%E5%A4%8D%E6%9D%82%E5%BA%A6/" title="Power method 估计复杂度"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://image-1302790708.cos.ap-chengdu.myqcloud.com/research.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-10-26</div><div class="title">Power method 估计复杂度</div></div></a></div><div><a href="/2022/11/16/Policy-gradient-method/" title="Policy gradient method"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://image-1302790708.cos.ap-chengdu.myqcloud.com/20221106220939.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-11-16</div><div class="title">Policy gradient method</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> Comment</span></div></div><div class="comment-wrap"><div><div id="gitalk-container"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/pics/loren.PNG" onerror="this.onerror=null;this.src='/pics/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">郑晓东</div><div class="author-info__description">男儿千年志，吾生未有涯</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">36</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">20</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">5</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/sheldon123z"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/sheldon123z" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:xiaodong_zheng@qq.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="https://www.linkedin.cn/injobs/in/xiaodong-zheng" target="_blank" title="LinkedIn"><i class="fab fa-linkedin"></i></a><a class="social-icon" href="/281309467" target="_blank" title="QQ"><i class="fab fa-qq"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">欢迎来到我的个人小站！</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#madrl"><span class="toc-number">1.</span> <span class="toc-text"> MADRL</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#abstract"><span class="toc-number">1.1.</span> <span class="toc-text"> Abstract</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#single-agent-deep-rl"><span class="toc-number">1.2.</span> <span class="toc-text"> Single Agent Deep RL</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#actor-critic"><span class="toc-number">1.2.1.</span> <span class="toc-text"> Actor-Critic</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#deep-q-network"><span class="toc-number">1.2.2.</span> <span class="toc-text"> “Deep Q-Network”</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#double-dqn-ddqn-%E5%8F%8Cdqn"><span class="toc-number">1.2.3.</span> <span class="toc-text"> “Double DQN (DDQN)” 双DQN</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#dueling-network-%E5%86%B3%E6%96%97%E7%BD%91%E7%BB%9C"><span class="toc-number">1.2.4.</span> <span class="toc-text"> “Dueling network” 决斗网络</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#deep-recurrent-q-network-drqn-%E6%B7%B1%E5%BA%A6%E9%80%92%E5%BD%92q%E7%BD%91%E7%BB%9C"><span class="toc-number">1.2.5.</span> <span class="toc-text"> “Deep recurrent Q-network (DRQN)” 深度递归Q网络</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#deep-attention-recurrent-q-network-darqn-%E6%B7%B1%E5%BA%A6%E5%85%B3%E6%B3%A8%E5%A4%8D%E5%8F%91q%E7%BD%91%E7%BB%9Cdarqn"><span class="toc-number">1.2.6.</span> <span class="toc-text"> “Deep attention recurrent Q-network (DARQN)” 深度关注复发Q网络（DARQN）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#multi-agent-deep-rl"><span class="toc-number">1.3.</span> <span class="toc-text"> Multi-agent Deep RL</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#nonstationarity-%E9%9D%9E%E5%B9%B3%E7%A8%B3%E6%80%A7"><span class="toc-number">1.3.1.</span> <span class="toc-text"> “Nonstationarity” :非平稳性</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#deep-repeated-update-q-network-druqn-%E6%B7%B1%E5%BA%A6%E9%87%8D%E5%A4%8D%E6%9B%B4%E6%96%B0q%E7%BD%91%E7%BB%9C"><span class="toc-number">1.3.1.1.</span> <span class="toc-text"> “deep repeated update Q-network (DRUQN)” 深度重复更新Q网络</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#deep-loosely-coupled-q-network-dlcqn-%E6%B7%B1%E5%BA%A6%E6%9D%BE%E8%80%A6%E5%90%88q%E7%BD%91%E7%BB%9C"><span class="toc-number">1.3.1.2.</span> <span class="toc-text"> “deep loosely coupled Q-network (DLCQN)” 深度松耦合Q网络</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#lenient-dqn-ldqn-%E4%BB%81%E6%85%88dqnldqn"><span class="toc-number">1.3.1.3.</span> <span class="toc-text"> “lenient-DQN (LDQN)”  仁慈DQN（LDQN）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#hystereticdqn-hdqn-%E6%BB%9E%E5%90%8Edqnhdqn"><span class="toc-number">1.3.1.4.</span> <span class="toc-text"> “hystereticDQN (HDQN)” 滞后DQN（HDQN）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#weighted-ddqn-%E5%8A%A0%E6%9D%83ddqn"><span class="toc-number">1.3.1.5.</span> <span class="toc-text"> “weighted DDQN” 加权DDQN</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#partial-observabilitypmod"><span class="toc-number">1.3.2.</span> <span class="toc-text"> “Partial Observability:”PMOD</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#drqn"><span class="toc-number">1.3.2.1.</span> <span class="toc-text"> DRQN</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#deep-distributed-recurrent-q-network-ddrqn-%E6%B7%B1%E5%88%86%E5%B8%83%E9%80%92%E5%BD%92q%E7%BD%91%E7%BB%9C"><span class="toc-number">1.3.2.2.</span> <span class="toc-text"> “Deep distributed recurrent Q-network (DDRQN)” 深分布递归Q网络</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#last-action-inputs"><span class="toc-number">1.3.2.3.</span> <span class="toc-text"> last-action inputs:</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#interagent-weight-sharing"><span class="toc-number">1.3.2.4.</span> <span class="toc-text"> interagent weight sharing:</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#disabling-experience-replay"><span class="toc-number">1.3.2.5.</span> <span class="toc-text"> disabling experience replay:</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#deep-policy-inference-q-network-dpiqn-%E6%B7%B1%E5%BA%A6%E7%AD%96%E7%95%A5%E6%8E%A8%E7%90%86q%E7%BD%91%E7%BB%9C"><span class="toc-number">1.3.2.6.</span> <span class="toc-text"> “Deep policy inference Q-network (DPIQN)” 深度策略推理Q网络</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#eep-recurrent-policy-inference-q-network-drpiqn-%E6%B7%B1%E5%BA%A6%E5%BE%AA%E7%8E%AF%E7%AD%96%E7%95%A5%E6%8E%A8%E7%90%86q%E7%BD%91%E7%BB%9Cdrpiqn"><span class="toc-number">1.3.2.7.</span> <span class="toc-text"> “eep recurrent policy inference Q-network (DRPIQN)” 深度循环策略推理Q网络（DRPIQN）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#multitask-marl-mt-marl-%E5%A4%9A%E4%BB%BB%E5%8A%A1marlmt-marl"><span class="toc-number">1.3.2.8.</span> <span class="toc-text"> “Multitask MARL (MT-MARL)” 多任务MARL（MT-MARL）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#maddpg-m"><span class="toc-number">1.3.2.9.</span> <span class="toc-text"> “MADDPG-M”</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#deep-deterministic-policy-gradient-ddpg-%E6%B7%B1%E5%BA%A6%E7%A1%AE%E5%AE%9A%E6%80%A7%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6"><span class="toc-number">1.3.2.10.</span> <span class="toc-text"> “Deep deterministic policy gradient (DDPG)” 深度确定性策略梯度</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#bayesian-action-decoder-bad-%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%8A%A8%E4%BD%9C%E8%A7%A3%E7%A0%81%E5%99%A8bad"><span class="toc-number">1.3.2.11.</span> <span class="toc-text"> “Bayesian action decoder (BAD)” 贝叶斯动作解码器（BAD）</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#mas-training-schemes"><span class="toc-number">1.3.3.</span> <span class="toc-text"> “MAS Training Schemes”</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#clde-centralized-learning-and-decentralized-execution-%E9%9B%86%E4%B8%AD%E5%AD%A6%E4%B9%A0%E5%92%8C%E5%88%86%E6%95%A3%E6%89%A7%E8%A1%8C"><span class="toc-number">1.3.3.1.</span> <span class="toc-text"> “CLDE Centralized Learning and Decentralized Execution” 集中学习和分散执行</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#centralized-learning"><span class="toc-number">1.3.3.1.1.</span> <span class="toc-text"> centralized learning：</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#decentralized-execution"><span class="toc-number">1.3.3.1.2.</span> <span class="toc-text"> decentralized execution:</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#centralized-learning-2"><span class="toc-number">1.3.3.2.</span> <span class="toc-text"> centralized learning</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#concurrent-learning-%E5%B9%B6%E8%A1%8C%E5%AD%A6%E4%B9%A0"><span class="toc-number">1.3.3.3.</span> <span class="toc-text"> concurrent learning 并行学习</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#centralized-learning-%E5%92%8C-concurrent-learning-%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="toc-number">1.3.3.4.</span> <span class="toc-text"> centralized learning 和 concurrent learning 的区别：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#parameter-sharing-%E5%8F%82%E6%95%B0%E5%85%B1%E4%BA%AB"><span class="toc-number">1.3.3.5.</span> <span class="toc-text"> “parameter sharing” 参数共享</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#parameter-sharing-and-trpo-%E5%8F%82%E6%95%B0%E5%85%B1%E4%BA%AB%E5%92%8Ctrpo"><span class="toc-number">1.3.3.6.</span> <span class="toc-text"> “Parameter Sharing and TRPO” 参数共享和TRPO</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#reinforced-interagent-learning-rial-%E5%BC%BA%E5%8C%96%E4%BA%A4%E4%BA%92%E5%AD%A6%E4%B9%A0rial"><span class="toc-number">1.3.3.7.</span> <span class="toc-text"> “Reinforced interagent learning (RIAL)” 强化交互学习（RIAL）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#differentiable-interagent-learning-dial-%E5%8F%AF%E5%BE%AE%E5%88%86%E4%BA%A4%E4%BA%92%E5%AD%A6%E4%B9%A0dial"><span class="toc-number">1.3.3.8.</span> <span class="toc-text"> “Differentiable interagent learning (DIAL)” 可微分交互学习（DIAL）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#deep-reinforcement-opponent-network-dron-%E6%B7%B1%E5%8A%A0%E5%9B%BA%E5%AF%B9%E6%89%8B%E7%BD%91%E7%BB%9C"><span class="toc-number">1.3.3.9.</span> <span class="toc-text"> “Deep reinforcement opponent network (DRON)” 深加固对手网络</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#multiagent-deep-deterministic-policy-gradient-maddpg-%E5%A4%9A%E4%BB%A3%E7%90%86%E6%B7%B1%E5%BA%A6%E7%A1%AE%E5%AE%9A%E6%80%A7%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6maddpg"><span class="toc-number">1.3.3.10.</span> <span class="toc-text"> “Multiagent deep deterministic policy gradient (MADDPG)” 多代理深度确定性策略梯度（MADDPG）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#counterfactual-multiagent-coma-%E5%8F%8D%E4%BA%8B%E5%AE%9E%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93"><span class="toc-number">1.3.3.11.</span> <span class="toc-text"> “Counterfactual multiagent (COMA)” 反事实多智能体</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#continuous-action-spaces-%E8%BF%9E%E7%BB%AD%E5%8A%A8%E4%BD%9C%E7%A9%BA%E9%97%B4"><span class="toc-number">1.3.4.</span> <span class="toc-text"> “Continuous Action Spaces” 连续动作空间</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#trust-region-policy-optimization-trpo-%E4%BF%A1%E4%BB%BB%E5%8C%BA%E5%9F%9F%E7%AD%96%E7%95%A5%E4%BC%98%E5%8C%96"><span class="toc-number">1.3.4.1.</span> <span class="toc-text"> “Trust region policy optimization (TRPO)” 信任区域策略优化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#deterministic-policy-gradient-dpg-%E7%A1%AE%E5%AE%9A%E6%80%A7%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6"><span class="toc-number">1.3.4.2.</span> <span class="toc-text"> “Deterministic policy gradient (DPG)” 确定性策略梯度</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#recurrent-dpg-rdpg-%E7%BB%8F%E5%B8%B8%E6%80%A7dpgrdpg"><span class="toc-number">1.3.4.3.</span> <span class="toc-text"> “Recurrent DPG (RDPG)” 经常性DPG（RDPG）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#ps-trpo"><span class="toc-number">1.3.4.4.</span> <span class="toc-text"> “PS-TRPO”</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#transfer-learning-for-madrl-madrl%E7%9A%84%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0"><span class="toc-number">1.3.5.</span> <span class="toc-text"> “Transfer Learning for MADRL” MADRL的迁移学习</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#policy-distillation-%E6%94%BF%E7%AD%96%E8%92%B8%E9%A6%8F"><span class="toc-number">1.3.5.1.</span> <span class="toc-text"> “Policy distillation” 政策蒸馏</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#progressive-neural-networks-%E6%B8%90%E8%BF%9B%E5%BC%8F%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">1.3.5.2.</span> <span class="toc-text"> “Progressive neural networks” 渐进式神经网络</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#actor-mimic-method-%E6%BC%94%E5%91%98%E6%A8%A1%E6%8B%9F%E6%B3%95"><span class="toc-number">1.3.5.3.</span> <span class="toc-text"> “Actor-mimic method” 演员模拟法</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#madrl-applications"><span class="toc-number">1.3.6.</span> <span class="toc-text"> MADRL Applications</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#sequential-social-dilemma-ssd-model-%E9%A1%BA%E5%BA%8F%E7%A4%BE%E4%BC%9A%E5%9B%B0%E5%A2%83ssd%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.3.6.1.</span> <span class="toc-text"> “Sequential social dilemma (SSD) model” 顺序社会困境（SSD）模型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#extension-of-matrix-game-social-dilemma-mgsd-%E7%9F%A9%E9%98%B5%E5%8D%9A%E5%BC%88%E7%A4%BE%E4%BC%9A%E5%9B%B0%E5%A2%83%E7%9A%84%E6%89%A9%E5%B1%95"><span class="toc-number">1.3.6.2.</span> <span class="toc-text"> “Extension of matrix game social dilemma (MGSD)” 矩阵博弈社会困境的扩展</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#swarm-systems-%E7%BE%A4%E7%B3%BB%E7%BB%9F"><span class="toc-number">1.3.6.3.</span> <span class="toc-text"> “swarm systems” 群系统</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#dueling-ddqn-dddqn-dueling-ddqndddqn"><span class="toc-number">1.3.6.4.</span> <span class="toc-text"> “Dueling DDQN (DDDQN)” Dueling DDQN（DDDQN）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#asynchronous-advantage-ac-a3c-%E5%BC%82%E6%AD%A5%E4%BC%98%E5%8A%BFaca3c"><span class="toc-number">1.3.6.5.</span> <span class="toc-text"> “Asynchronous advantage AC (A3C)” 异步优势AC（A3C）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#commnet-model-commnet%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.3.6.6.</span> <span class="toc-text"> “CommNet model” CommNet模型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#contextual-deep-q-learning-%E4%B8%8A%E4%B8%8B%E6%96%87%E6%B7%B1%E5%B1%82q-%E5%AD%A6%E4%B9%A0"><span class="toc-number">1.3.6.7.</span> <span class="toc-text"> “Contextual deep Q-learning” 上下文深层Q-学习</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#contextual-multiagent-ac-%E4%B8%8A%E4%B8%8B%E6%96%87%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93ac"><span class="toc-number">1.3.6.8.</span> <span class="toc-text"> “Contextual multiagent AC” 上下文多智能体AC</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#future-challenges-research-directions"><span class="toc-number">1.4.</span> <span class="toc-text"> Future Challenges &amp; Research Directions</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#imitation-learning-%E6%A8%A1%E4%BB%BF%E5%AD%A6%E4%B9%A0"><span class="toc-number">1.4.1.</span> <span class="toc-text"> “Imitation learning” 模仿学习</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#inverse-rl-%E5%8F%8D%E5%90%91rl"><span class="toc-number">1.4.2.</span> <span class="toc-text"> “Inverse RL” 反向RL</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#human-on-the-loop"><span class="toc-number">1.4.3.</span> <span class="toc-text"> “Human-on-the-loop”</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#scaling-to-large-systems"><span class="toc-number">1.4.4.</span> <span class="toc-text"> “Scaling to large systems”</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#homogeneous-agents-%E5%90%8C%E8%B4%A8%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93"><span class="toc-number">1.4.4.1.</span> <span class="toc-text"> Homogeneous agents 同质多智能体</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#heterogeneous-agents-%E5%BC%82%E8%B4%A8%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93"><span class="toc-number">1.4.4.2.</span> <span class="toc-text"> Heterogeneous agents 异质多智能体</span></a></li></ol></li></ol></li></ol></li></ol></div></div><div class="card-widget" id="card-poem"><div id="poem_sentence"></div><div id="poem_info"><div id="poem_dynasty"></div><div id="poem_author"></div></div></div><script src="https://cdn.jsdelivr.net/npm/js-heo@1.0.11/poem/jinrishici.js" charset="utf-8"></script><script type="text/javascript">jinrishici.load(function(result) {
var sentence = document.querySelector("#poem_sentence")
var author = document.querySelector("#poem_author")
var dynasty = document.querySelector("#poem_dynasty")

var sentenceText = result.data.content
sentenceText = sentenceText.substr(0, sentenceText.length - 1);
sentence.innerHTML = sentenceText
dynasty.innerHTML = result.data.origin.dynasty
author.innerHTML = result.data.origin.author + '《' + result.data.origin.title + '》'
});</script><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2023/06/05/%E5%A5%BD%E6%B6%88%E6%81%AF-%E6%88%91%E6%8B%8D%E4%BA%86%E5%A9%9A%E7%BA%B1%E7%85%A7/" title="好消息,我拍了婚纱照"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://image-1302790708.cos.ap-chengdu.myqcloud.com/loveandsunset.png" onerror="this.onerror=null;this.src='/pics/404.jpg'" alt="好消息,我拍了婚纱照"/></a><div class="content"><a class="title" href="/2023/06/05/%E5%A5%BD%E6%B6%88%E6%81%AF-%E6%88%91%E6%8B%8D%E4%BA%86%E5%A9%9A%E7%BA%B1%E7%85%A7/" title="好消息,我拍了婚纱照">好消息,我拍了婚纱照</a><time datetime="2023-06-05T13:23:38.000Z" title="Created 2023-06-05 21:23:38">2023-06-05</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/05/06/%E5%A5%BD%E6%B6%88%E6%81%AF-%E6%88%91%E8%AE%A2%E5%A9%9A%E4%BA%86/" title="好消息,我订婚了！"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://image-1302790708.cos.ap-chengdu.myqcloud.com/wujiayouxi.jpeg" onerror="this.onerror=null;this.src='/pics/404.jpg'" alt="好消息,我订婚了！"/></a><div class="content"><a class="title" href="/2023/05/06/%E5%A5%BD%E6%B6%88%E6%81%AF-%E6%88%91%E8%AE%A2%E5%A9%9A%E4%BA%86/" title="好消息,我订婚了！">好消息,我订婚了！</a><time datetime="2023-05-06T10:49:21.000Z" title="Created 2023-05-06 18:49:21">2023-05-06</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/11/26/11-24/" title="11-24"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://image-1302790708.cos.ap-chengdu.myqcloud.com/%E6%82%BC%E5%BF%B5.png" onerror="this.onerror=null;this.src='/pics/404.jpg'" alt="11-24"/></a><div class="content"><a class="title" href="/2022/11/26/11-24/" title="11-24">11-24</a><time datetime="2022-11-26T03:09:47.000Z" title="Created 2022-11-26 11:09:47">2022-11-26</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/11/20/On-Policy-Approximation/" title="On Policy Approximation"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://image-1302790708.cos.ap-chengdu.myqcloud.com/research.png" onerror="this.onerror=null;this.src='/pics/404.jpg'" alt="On Policy Approximation"/></a><div class="content"><a class="title" href="/2022/11/20/On-Policy-Approximation/" title="On Policy Approximation">On Policy Approximation</a><time datetime="2022-11-20T05:19:43.000Z" title="Created 2022-11-20 13:19:43">2022-11-20</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/11/16/cs234-4/" title="cs234-4: SARSA、Q-learning、On policy 和off policy简单理解"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://image-1302790708.cos.ap-chengdu.myqcloud.com/20221106220939.png" onerror="this.onerror=null;this.src='/pics/404.jpg'" alt="cs234-4: SARSA、Q-learning、On policy 和off policy简单理解"/></a><div class="content"><a class="title" href="/2022/11/16/cs234-4/" title="cs234-4: SARSA、Q-learning、On policy 和off policy简单理解">cs234-4: SARSA、Q-learning、On policy 和off policy简单理解</a><time datetime="2022-11-16T07:27:07.000Z" title="Created 2022-11-16 15:27:07">2022-11-16</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('https://image-1302790708.cos.ap-chengdu.myqcloud.com/20221106221145.png')"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2023 By 郑晓东</div><div class="footer_custom_text"><a target="_blank" rel="noopener" href="https://beian.miit.gov.cn/"><img class="icp-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="icp圖片"><span>备案号：陕ICP备2022004677号-1</span></a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="Toggle Between Traditional Chinese And Simplified Chinese">繁</button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="Scroll To Comments"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="Back To Top"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container').forEach(node => {
            if (node.hasAttribute('display')) {
              btf.wrap(node, 'div', { class: 'mathjax-overflow' })
            } else {
              btf.wrap(node, 'span', { class: 'mathjax-overflow' })
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script><script>(() => {
  const $mermaidWrap = document.querySelectorAll('#article-container .mermaid-wrap')
  if ($mermaidWrap.length) {
    window.runMermaid = () => {
      window.loadMermaid = true
      const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? '' : ''

      Array.from($mermaidWrap).forEach((item, index) => {
        const mermaidSrc = item.firstElementChild
        const mermaidThemeConfig = '%%{init:{ \'theme\':\'' + theme + '\'}}%%\n'
        const mermaidID = 'mermaid-' + index
        const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent
        mermaid.mermaidAPI.render(mermaidID, mermaidDefinition, (svgCode) => {
          mermaidSrc.insertAdjacentHTML('afterend', svgCode)
        })
      })
    }

    const loadMermaid = () => {
      window.loadMermaid ? runMermaid() : getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(runMermaid)
    }

    window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
  }
})()</script><script>function addGitalkSource () {
  const ele = document.createElement('link')
  ele.rel = 'stylesheet'
  ele.href= 'https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css'
  document.getElementsByTagName('head')[0].appendChild(ele)
}

function loadGitalk () {
  function initGitalk () {
    var gitalk = new Gitalk(Object.assign({
      clientID: '8b9b962d4caf1803debb',
      clientSecret: 'ca4485d0c81e59d947639fcf41103ca90306b2eb',
      repo: 'gitalk_comment',
      owner: 'sheldon123z',
      admin: ['sheldon123z'],
      id: '719b2446f7f6af4eec121bb319a998dc',
      updateCountCallback: commentCount
    },null))

    gitalk.render('gitalk-container')
  }

  if (typeof Gitalk === 'function') initGitalk()
  else {
    addGitalkSource()
    getScript('https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.js').then(initGitalk)
  }
}

function commentCount(n){
  let isCommentCount = document.querySelector('#post-meta .gitalk-comment-count')
  if (isCommentCount) {
    isCommentCount.innerHTML= n
  }
}

if ('Gitalk' === 'Gitalk' || !true) {
  if (true) btf.loadComment(document.getElementById('gitalk-container'), loadGitalk)
  else loadGitalk()
} else {
  function loadOtherComment () {
    loadGitalk()
  }
}</script></div><div class="aplayer no-destroy" data-id="323070469" data-server="netease" data-type="playlist" data-fixed="true" data-mini="true" data-listFolded="false" data-order="random" data-preload="auto" data-autoplay="false" data-mutex="true" data-volume="0.7"></div><script async data-pjax src="/js/custom.js"></script><script data-pjax src="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/js/swiper.min.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/metingjs/dist/Meting.min.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax/pjax.min.js"></script><script>let pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]

var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener scroll 
  window.tocScrollFn && window.removeEventListener('scroll', window.tocScrollFn)
  window.scrollCollect && window.removeEventListener('scroll', scrollCollect)

  document.getElementById('rightside').style.cssText = "opacity: ''; transform: ''"
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

  typeof disqusjs === 'object' && disqusjs.destroy()
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof chatBtnFn === 'function' && chatBtnFn()
  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()
})

document.addEventListener('pjax:error', (e) => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><!-- hexo injector body_end start --> <script data-pjax>if(document.getElementById('recent-posts') && (location.pathname ==='/'|| '/' ==='all')){
    var parent = document.getElementById('recent-posts');
    var child = '<div class="recent-post-item" style="width:100%;height: auto; border: 2px solid blue"><div id="catalog_magnet"><div class="magnet_item"><a class="magnet_link" href="http://www.zhengxiaodong.com/categories/科研/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🧑‍🎓📚 神圣科研 (7)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="http://www.zhengxiaodong.com/categories/笔记/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🗒️ 学习笔记 (16)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="http://www.zhengxiaodong.com/categories/日志/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🐶 生活记录 (9)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="http://www.zhengxiaodong.com/categories/随笔/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">📖 读点闲书 (1)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><a class="magnet_link_more"  href="http://www.zhengxiaodong.com/categories" style="flex:1;text-align: center;margin-bottom: 10px;">查看更多...</a></div></div>';
    console.log('已挂载magnet')
    parent.insertAdjacentHTML("afterbegin",child)}
     </script><style>#catalog_magnet{flex-wrap: wrap;display: flex;width:100%;justify-content:space-between;padding: 10px 10px 0 10px;align-content: flex-start;}.magnet_item{flex-basis: calc(50% - 5px);background: #f2f2f2;margin-bottom: 10px;border-radius: 8px;transition: all 0.2s ease-in-out;}.magnet_item:hover{background: #2881ED}.magnet_link_more{color:#555}.magnet_link{color:black}.magnet_link:hover{color:white}@media screen and (max-width: 600px) {.magnet_item {flex-basis: 100%;}}.magnet_link_context{display:flex;padding: 10px;font-size:16px;transition: all 0.2s ease-in-out;}.magnet_link_context:hover{padding: 10px 20px;}</style>
    <style></style><script data-pjax>
  function butterfly_clock_injector_config(){
    var parent_div_git = document.getElementsByClassName('sticky_layout')[0];
    var item_html = '<div class="card-widget card-clock"><div class="card-glass"><div class="card-background"><div class="card-content"><div id="hexo_electric_clock"><img class="entered loading" id="card-clock-loading" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://unpkg.zhimg.com/hexo-butterfly-clock/lib/loading.gif" style="height: 120px; width: 100%;" data-ll-status="loading"/></div></div></div></div></div>';
    console.log('已挂载butterfly_clock')
    parent_div_git.insertAdjacentHTML("afterbegin",item_html)
    }
  var elist = '/posts/,/about/'.split(',');
  var cpage = location.pathname;
  var epage = 'all';
  var flag = 0;

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_clock_injector_config();
  }
  else if (epage === cpage){
    butterfly_clock_injector_config();
  }
  </script><script src="https://pv.sohu.com/cityjson?ie=utf-8"></script><script data-pjax src="https://unpkg.zhimg.com/hexo-butterfly-clock/lib/clock.min.js"></script><script data-pjax>
  function butterfly_swiper_injector_config(){
    var parent_div_git = document.getElementById('recent-posts');
    var item_html = '<div class="swiper_container_card" style="height: auto;width: 100%"><div id="bannerGroup"><div id="random"><div id="random-banner"><canvas id="peoplecanvas"></canvas></div><a id="random-hover" style="width:100%;height:auto;" href="javascript:toRandomPost()" rel="external nofollow noreferrer" one-link-mark="yes"><i class="fa fa-paper-plane" style="margin-left:10px"></i><div style="margin-left:10px">随便逛逛<i class="fa-solid fa-arrow-right" style="margin-left:10px"></i></div></a></div><div class="categoryGroup"><div class="categoryItem" style="box-shadow:#fefeff"><a class="categoryButton blue" onclick="pjax.loadUrl(&quot;/music/&quot;);" href="javascript:void(0);"><span class="categoryButtonText">音乐</span><i class="fas fa-music"></i></a></div><div class="categoryItem" style="box-shadow:#fefeff"><a class="categoryButton green" onclick="pjax.loadUrl(&quot;/Gallery/&quot;);" href="javascript:void(0);"><span class="categoryButtonText">照片</span><i class="fa-regular fa-images"></i></a></div><div class="categoryItem" style="box-shadow:#fefeff"><a class="categoryButton red" onclick="pjax.loadUrl(&quot;/projects/&quot;);" href="javascript:void(0);"><span class="categoryButtonText">项目</span><i class="fas fa-robot"></i></a></div></div></div><div id="swiper_container_blog"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" onclick="pjax.loadUrl(&quot;2022/10/26/Alpha-alpha-Rank/&quot;);" href="javascript:void(0);" title="Alpha-alpha-Rank"><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://image-1302790708.cos.ap-chengdu.myqcloud.com/20221106221319.png" alt="图片" onerror="this.src=https://cdn.cbd.int/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2022-10-26</span><a class="blog-slider__title" onclick="pjax.loadUrl(&quot;2022/10/26/Alpha-alpha-Rank/&quot;);" href="javascript:void(0);" alt="">Alpha-alpha-Rank</a><div class="blog-slider__text">论文&lt;Many-Agent Reinforcement Learning&gt; By Yaodong YANG</div><a class="blog-slider__button" onclick="pjax.loadUrl(&quot;2022/10/26/Alpha-alpha-Rank/&quot;);" href="javascript:void(0);" alt="">详情       </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" onclick="pjax.loadUrl(&quot;2022/11/06/cs234-2/&quot;);" href="javascript:void(0);" title="cs234-2: 马尔科夫奖励过程、Policy improvement"><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://image-1302790708.cos.ap-chengdu.myqcloud.com/20221106220835.png" alt="图片" onerror="this.src=https://cdn.cbd.int/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2022-11-06</span><a class="blog-slider__title" onclick="pjax.loadUrl(&quot;2022/11/06/cs234-2/&quot;);" href="javascript:void(0);" alt="">cs234-2: 马尔科夫奖励过程、Policy improvement</a><div class="blog-slider__text">一些基础概念</div><a class="blog-slider__button" onclick="pjax.loadUrl(&quot;2022/11/06/cs234-2/&quot;);" href="javascript:void(0);" alt="">详情       </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" onclick="pjax.loadUrl(&quot;2023/05/06/好消息-我订婚了/&quot;);" href="javascript:void(0);" title="好消息,我订婚了！"><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://image-1302790708.cos.ap-chengdu.myqcloud.com/wujiayouxi.jpeg" alt="图片" onerror="this.src=https://cdn.cbd.int/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-05-06</span><a class="blog-slider__title" onclick="pjax.loadUrl(&quot;2023/05/06/好消息-我订婚了/&quot;);" href="javascript:void(0);" alt="">好消息,我订婚了！</a><div class="blog-slider__text">特大喜讯！</div><a class="blog-slider__button" onclick="pjax.loadUrl(&quot;2023/05/06/好消息-我订婚了/&quot;);" href="javascript:void(0);" alt="">详情       </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" onclick="pjax.loadUrl(&quot;2023/06/05/好消息-我拍了婚纱照/&quot;);" href="javascript:void(0);" title="好消息,我拍了婚纱照"><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://image-1302790708.cos.ap-chengdu.myqcloud.com/loveandsunset.png" alt="图片" onerror="this.src=https://cdn.cbd.int/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-06-05</span><a class="blog-slider__title" onclick="pjax.loadUrl(&quot;2023/06/05/好消息-我拍了婚纱照/&quot;);" href="javascript:void(0);" alt="">好消息,我拍了婚纱照</a><div class="blog-slider__text">特大喜讯！</div><a class="blog-slider__button" onclick="pjax.loadUrl(&quot;2023/06/05/好消息-我拍了婚纱照/&quot;);" href="javascript:void(0);" alt="">详情       </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div><div id="topGroup"><div class="topGroup"><div class="top-group-list-item"><div class="post_cover left_radius"><a onclick="pjax.loadUrl(&quot;2022/10/26/MADRL总结/&quot;);" href="javascript:void(0);" alt="" title="MADRL总结" data-pjax-state="data-pjax-state"><span class="top-group-text">荐</span><img class="post_bg" alt="MADRL总结" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://image-1302790708.cos.ap-chengdu.myqcloud.com/20221106221145.png" onerror="this.src=https://cdn.cbd.int/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a></div><div class="top-group-info"><a class="article-title" onclick="pjax.loadUrl(&quot;2022/10/26/MADRL总结/&quot;);" href="javascript:void(0);" alt="" title="MADRL总结" data-pjax-state="data-pjax-state">MADRL总结</a></div></div><div class="top-group-list-item"><div class="post_cover left_radius"><a onclick="pjax.loadUrl(&quot;2023/05/06/好消息-我订婚了/&quot;);" href="javascript:void(0);" alt="" title="好消息,我订婚了！" data-pjax-state="data-pjax-state"><span class="top-group-text">荐</span><img class="post_bg" alt="好消息,我订婚了！" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://image-1302790708.cos.ap-chengdu.myqcloud.com/wujiayouxi.jpeg" onerror="this.src=https://cdn.cbd.int/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a></div><div class="top-group-info"><a class="article-title" onclick="pjax.loadUrl(&quot;2023/05/06/好消息-我订婚了/&quot;);" href="javascript:void(0);" alt="" title="好消息,我订婚了！" data-pjax-state="data-pjax-state">好消息,我订婚了！</a></div></div><div class="top-group-list-item"><div class="post_cover left_radius"><a onclick="pjax.loadUrl(&quot;2020/11/10/佛罗里达的沙/&quot;);" href="javascript:void(0);" alt="" title="佛罗里达的沙" data-pjax-state="data-pjax-state"><span class="top-group-text">荐</span><img class="post_bg" alt="佛罗里达的沙" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://image-1302790708.cos.ap-chengdu.myqcloud.com/5f19f82b97b64e4a9a98c93da508c62.jpg" onerror="this.src=https://cdn.cbd.int/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a></div><div class="top-group-info"><a class="article-title" onclick="pjax.loadUrl(&quot;2020/11/10/佛罗里达的沙/&quot;);" href="javascript:void(0);" alt="" title="佛罗里达的沙" data-pjax-state="data-pjax-state">佛罗里达的沙</a></div></div><div class="top-group-list-item"><div class="post_cover left_radius"><a onclick="pjax.loadUrl(&quot;2022/03/02/我的女孩/&quot;);" href="javascript:void(0);" alt="" title="我的女孩" data-pjax-state="data-pjax-state"><span class="top-group-text">荐</span><img class="post_bg" alt="我的女孩" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="2022/03/02/我的女孩/cover.jpg" onerror="this.src=https://cdn.cbd.int/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a></div><div class="top-group-info"><a class="article-title" onclick="pjax.loadUrl(&quot;2022/03/02/我的女孩/&quot;);" href="javascript:void(0);" alt="" title="我的女孩" data-pjax-state="data-pjax-state">我的女孩</a></div></div><div class="top-group-list-item"><div class="post_cover left_radius"><a onclick="pjax.loadUrl(&quot;2023/06/05/好消息-我拍了婚纱照/&quot;);" href="javascript:void(0);" alt="" title="好消息,我拍了婚纱照" data-pjax-state="data-pjax-state"><span class="top-group-text">荐</span><img class="post_bg" alt="好消息,我拍了婚纱照" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://image-1302790708.cos.ap-chengdu.myqcloud.com/loveandsunset.png" onerror="this.src=https://cdn.cbd.int/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a></div><div class="top-group-info"><a class="article-title" onclick="pjax.loadUrl(&quot;2023/06/05/好消息-我拍了婚纱照/&quot;);" href="javascript:void(0);" alt="" title="好消息,我拍了婚纱照" data-pjax-state="data-pjax-state">好消息,我拍了婚纱照</a></div></div><div class="top-group-list-none"></div><div class="top-group-list-none"></div><div class="top-group-list-none"></div></div></div></div></div>';
    console.log('已挂载butterfly_swiper')
    parent_div_git.insertAdjacentHTML("afterbegin",item_html)
    }
  var elist = 'undefined'.split(',');
  var cpage = location.pathname;
  var epage = 'all';
  var flag = 0;

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_swiper_injector_config();
  }
  else if (epage === cpage){
    butterfly_swiper_injector_config();
  }
  </script><script defer src="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/js/swiper.min.js"></script><script defer data-pjax src="https://cdn.cbd.int/hexo-butterfly-swiper-anzhiyu-pro/lib/swiper_init.js"></script><script data-pjax src="https://lf26-cdn-tos.bytecdntp.com/cdn/expire-1-M/gsap/3.9.1/gsap.min.js"></script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper-anzhiyu-pro/lib/people.min.js"></script><script async src="/anzhiyu/random.js"></script><!-- hexo injector body_end end --></body></html>